# List of publications using Lingvo.



## Translation

<!-- This document was automatically generated with bibtex2html 1.98
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     /usr/bin/bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="freitag2019text">1</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Freitag, I.&nbsp;Caswell, and S.&nbsp;Roy, &ldquo;Text Repair Model for Neural
  Machine Translation,&rdquo; <em>arXiv e-prints</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1904.04790">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chen2018best">2</a>]
</td>
<td class="bibtexitem">
M.&nbsp;X. Chen, O.&nbsp;Firat, A.&nbsp;Bapna, M.&nbsp;Johnson, W.&nbsp;Macherey, G.&nbsp;Foster, L.&nbsp;Jones,
  M.&nbsp;Schuster, N.&nbsp;Shazeer, N.&nbsp;Parmar, A.&nbsp;Vaswani, J.&nbsp;Uszkoreit, L.&nbsp;Kaiser,
  Z.&nbsp;Chen, Y.&nbsp;Wu, and M.&nbsp;Hughes, &ldquo;The Best of Both Worlds: Combining Recent
  Advances in Neural Machine Translation,&rdquo; in <em>Proc. Annual Meeting of
  the Association for Computational Linguistics (ACL)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1804.09849">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="cherry2018revisiting">3</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Cherry, G.&nbsp;Foster, A.&nbsp;Bapna, O.&nbsp;Firat, and W.&nbsp;Macherey, &ldquo;Revisiting
  character-based neural machine translation with capacity and compression,&rdquo;
  in <em>Proc. Conference on Empirical Methods in Natural Language Processing
  (EMNLP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1808.09943">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bapna2018training">4</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Bapna, M.&nbsp;X. Chen, O.&nbsp;Firat, Y.&nbsp;Cao, and Y.&nbsp;Wu, &ldquo;Training deeper neural
  machine translation models with transparent attention,&rdquo; in <em>Proc.
  Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>,
  2018.
[&nbsp;<a href="https://arxiv.org/abs/1808.07561">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wu2016google">5</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Wu, M.&nbsp;Schuster, Z.&nbsp;Chen, Q.&nbsp;V. Le, M.&nbsp;Norouzi, W.&nbsp;Macherey, M.&nbsp;Krikun,
  Y.&nbsp;Cao, Q.&nbsp;Gao, K.&nbsp;Macherey, J.&nbsp;Klingner, A.&nbsp;Shah, M.&nbsp;Johnson, X.&nbsp;Liu,
  L.&nbsp;Kaiser, S.&nbsp;Gouws, Y.&nbsp;Kato, T.&nbsp;Kudo, H.&nbsp;Kazawa, K.&nbsp;Stevens, G.&nbsp;Kurian,
  N.&nbsp;Patil, W.&nbsp;Wang, C.&nbsp;Young, J.&nbsp;Smith, J.&nbsp;Riesa, A.&nbsp;Rudnick, O.&nbsp;Vinyals,
  G.&nbsp;Corrado, M.&nbsp;Hughes, and J.&nbsp;Dean, &ldquo;Google's neural machine translation
  system: Bridging the gap between human and machine translation,&rdquo; tech. rep.,
  2016.
[&nbsp;<a href="https://arxiv.org/abs/1609.08144">pdf</a>&nbsp;]

</td>
</tr>
</table>

## Speech recognition

<!-- This document was automatically generated with bibtex2html 1.98
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     /usr/bin/bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chiu2018state">1</a>]
</td>
<td class="bibtexitem">
C.-C.Chiu, T.&nbsp;N. Sainath, Y.&nbsp;Wu, R.&nbsp;Prabhavalkar, P.&nbsp;Nguyen, Z.&nbsp;Chen,
  A.&nbsp;Kannan, R.&nbsp;J. Weiss, K.&nbsp;Rao, K.&nbsp;Gonina, N.&nbsp;Jaitly, B.&nbsp;Li, J.&nbsp;Chorowski,
  and M.&nbsp;Bacchiani, &ldquo;State-of-the-art speech recognition with
  sequence-to-sequence models,&rdquo; in <em>Proc. IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01769">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="toshniwal2018multilingual">2</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Toshniwal, T.&nbsp;N. Sainath, R.&nbsp;J. Weiss, B.&nbsp;Li, P.&nbsp;Moreno, E.&nbsp;Weinstein, and
  K.&nbsp;Rao, &ldquo;Multilingual speech recognition with a single end-to-end model,&rdquo;
  in <em>Proc. IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1711.01694">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="li2018multidialect">3</a>]
</td>
<td class="bibtexitem">
B.&nbsp;Li, T.&nbsp;N. Sainath, K.&nbsp;Sim, M.&nbsp;Bacchiani, E.&nbsp;Weinstein, P.&nbsp;Nguyen, Z.&nbsp;Chen,
  Y.&nbsp;Wu, and K.&nbsp;Rao, &ldquo;Multi-Dialect Speech Recognition With a Single
  Sequence-to-Sequence Model,&rdquo; in <em>Proc. IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01541">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sainath2018no">4</a>]
</td>
<td class="bibtexitem">
T.&nbsp;N. Sainath, P.&nbsp;Prabhavalkar, S.&nbsp;Kumar, S.&nbsp;Lee, A.&nbsp;Kannan, D.&nbsp;Rybach,
  V.&nbsp;Schogol, P.&nbsp;Nguyen, B.&nbsp;Li, Y.&nbsp;Wu, Z.&nbsp;Chen, and C.&nbsp;C. Chiu, &ldquo;No Need for
  a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End
  Models,&rdquo; in <em>Proc. IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01864">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="lawson2018learning">5</a>]
</td>
<td class="bibtexitem">
D.&nbsp;Lawson, C.&nbsp;C. Chiu, G.&nbsp;Tucker, C.&nbsp;Raffel, K.&nbsp;Swersky, and N.&nbsp;Jaitly,
  &ldquo;Learning hard alignments with variational inference,&rdquo; in <em>Proc.
  IEEE International Conference on Acoustics, Speech, and Signal Processing
  (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1705.05524">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kannan2018analysis">6</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Kannan, Y.&nbsp;Wu, P.&nbsp;Nguyen, T.&nbsp;N. Sainath, Z.&nbsp;Chen, and R.&nbsp;Prabhavalkar, &ldquo;An
  analysis of incorporating an external language model into a
  sequence-to-sequence model,&rdquo; in <em>Proc. IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01996">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="prabhavalkar2018minimum">7</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Prabhavalkar, T.&nbsp;N. Sainath, Y.&nbsp;Wu, P.&nbsp;Nguyen, Z.&nbsp;Chen, C.&nbsp;C. Chiu, and
  A.&nbsp;Kannan, &ldquo;Minimum Word Error Rate Training for Attention-based
  Sequence-to-sequence Models,&rdquo; in <em>Proc. IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01818">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sainath2018improving">8</a>]
</td>
<td class="bibtexitem">
T.&nbsp;N. Sainath, C.&nbsp;C. Chiu, R.&nbsp;Prabhavalkar, A.&nbsp;Kannan, Y.&nbsp;Wu, P.&nbsp;Nguyen, and
  Z.&nbsp;C. Z, &ldquo;Improving the Performance of Online Neural Transducer Models,&rdquo;
  in <em>Proc. IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01807">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chiu2018monotonic">9</a>]
</td>
<td class="bibtexitem">
C.&nbsp;C. Chiu and C.&nbsp;Raffel, &ldquo;Monotonic Chunkwise Attention,&rdquo; in <em>Proc.
  International Conference on Learning Representations (ICLR)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.05382">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="williams2018contextual">10</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Williams, A.&nbsp;Kannan, P.&nbsp;Aleksic, D.&nbsp;Rybach, and T.&nbsp;N.&nbsp;S. TN, &ldquo;Contextual
  Speech Recognition in End-to-End Neural Network Systems using Beam Search,&rdquo;
  in <em>Proc. Interspeech</em>, 2018.
[&nbsp;<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/2416.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chui2018speech">11</a>]
</td>
<td class="bibtexitem">
C.&nbsp;C. Chiu, A.&nbsp;Tripathi, K.&nbsp;Chou, C.&nbsp;Co, N.&nbsp;Jaitly, D.&nbsp;Jaunzeikare, A.&nbsp;Kannan,
  P.&nbsp;Nguyen, H.&nbsp;Sak, A.&nbsp;Sankar, J.&nbsp;Tansuwan, N.&nbsp;Wan, Y.&nbsp;Wu, and X.&nbsp;Zhang,
  &ldquo;Speech recognition for medical conversations,&rdquo; in <em>Proc.
  Interspeech</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1711.07274">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="pang2018compression">12</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Pang, T.&nbsp;N. Sainath, R.&nbsp;Prabhavalkar, S.&nbsp;Gupta, Y.&nbsp;Wu, S.&nbsp;Zhang, and C.&nbsp;C.
  Chiu, &ldquo;Compression of End-to-End Models,&rdquo; in <em>Proc. Interspeech</em>,
  2018.
[&nbsp;<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1025.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="toshniwal2018comparison">13</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Toshniwal, A.&nbsp;Kannan, C.&nbsp;C. Chiu, Y.&nbsp;Wu, T.&nbsp;N. Sainath, and K.&nbsp;Livescu, &ldquo;A
  comparison of techniques for language model integration in encoder-decoder
  speech recognition,&rdquo; in <em>Proc. IEEE Spoken Language Technology
  Workshop (SLT)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1807.10857">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="pundak2018deep">14</a>]
</td>
<td class="bibtexitem">
G.&nbsp;Pundak, T.&nbsp;N. Sainath, R.&nbsp;Prabhavalkar, A.&nbsp;Kannan, and D.&nbsp;Zhao, &ldquo;Deep
  context: End-to-end contextual speech recognition,&rdquo; in <em>Proc. IEEE
  Spoken Language Technology Workshop (SLT)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1808.02480">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="li2019bytes">15</a>]
</td>
<td class="bibtexitem">
B.&nbsp;Li, Y.&nbsp;Zhang, T.&nbsp;N. Sainath, Y.&nbsp;Wu, and W.&nbsp;Chan, &ldquo;Bytes are all you need:
  End-to-end multilingual speech recognition and synthesis with bytes,&rdquo; in
  <em>Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1811.09021">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="guo2019spelling">16</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Guo, T.&nbsp;N. Sainath, and R.&nbsp;J. Weiss, &ldquo;A spelling correction model for
  end-to-end speech recognition,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1902.07178">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="alon2019contextual">17</a>]
</td>
<td class="bibtexitem">
U.&nbsp;Alon, G.&nbsp;Pundak, and T.&nbsp;N. Sainath, &ldquo;Contextual speech recognition with
  difficult negative training examples,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1810.12170">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="qin2019imperceptible">18</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Qin, N.&nbsp;Carlini, I.&nbsp;Goodfellow, G.&nbsp;Cottrell, and C.&nbsp;Raffel, &ldquo;Imperceptible,
  robust, and targeted adversarial examples for automatic speech recognition,&rdquo;
  in <em>Proc. International Conference on Machine Learning (ICML)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1903.10346">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="park2019specaugment">19</a>]
</td>
<td class="bibtexitem">
D.&nbsp;S. Park, W.&nbsp;Chan, Y.&nbsp;Zhang, C.&nbsp;Chiu, B.&nbsp;Zoph, E.&nbsp;D. Cubuk, and Q.&nbsp;V. Le,
  &ldquo;SpecAugment: A Simple Data Augmentation Method for Automatic Speech
  Recognition,&rdquo; in <em>arXiv</em>, 2019.
[&nbsp;<a href="https://arxiv.org/pdf/1904.08779.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="li2019semisupervised">20</a>]
</td>
<td class="bibtexitem">
B.&nbsp;Li, T.&nbsp;N. Sainath, R.&nbsp;Pang, and Z.&nbsp;Wu, &ldquo;Semi-supervised training for
  end-to-end models via weak distillation,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://ieeexplore.ieee.org/document/8682172">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chang2019joint">21</a>]
</td>
<td class="bibtexitem">
S.-Y. Chang, R.&nbsp;Prabhavalkar, Y.&nbsp;He, T.&nbsp;N. Sainath, and G.&nbsp;Simko, &ldquo;Joint
  endpointing and decoding with end-to-end models,&rdquo; in <em>Proc. IEEE
  International Conference on Acoustics, Speech, and Signal Processing
  (ICASSP)</em>, 2019.
[&nbsp;<a href="https://ieeexplore.ieee.org/document/8683109">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="heymann2019improving">22</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Heymann, K.&nbsp;C. Sim, and B.&nbsp;Li, &ldquo;Improving ctc using stimulated learning for
  sequence modeling,&rdquo; in <em>Proc. IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://ieeexplore.ieee.org/document/8682700">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bruguier2019phoebe">23</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Bruguier, R.&nbsp;Prabhavalkar, G.&nbsp;Pundak, and T.&nbsp;N. Sainath, &ldquo;Phoebe:
  Pronunciation-aware contextualization for end-to-end speech recognition,&rdquo; in
  <em>Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://ieeexplore.ieee.org/document/8682441">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="he2019streaming">24</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;He, T.&nbsp;N. Sainath, R.&nbsp;Prabhavalkar, I.&nbsp;McGraw, R.&nbsp;Alvarez, D.&nbsp;Zhao,
  D.&nbsp;Rybach, A.&nbsp;Kannan, Y.&nbsp;Wu, R.&nbsp;Pang, Q.&nbsp;Liang, D.&nbsp;Bhatia, Y.&nbsp;Shangguan,
  B.&nbsp;Li, G.&nbsp;Pundak, K.&nbsp;C. Sim, T.&nbsp;Bagby, S.-Y. Chang, K.&nbsp;Rao, and
  A.&nbsp;Gruenstein, &ldquo;Streaming end-to-end speech recognition for mobile
  devices,&rdquo; in <em>Proc. IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1811.06621">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="irie2019unit">25</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Irie, R.&nbsp;Prabhavalkar, A.&nbsp;Kannan, A.&nbsp;Bruguier, D.&nbsp;Rybach, and P.&nbsp;Nguyen,
  &ldquo;Model unit exploration for sequence-to-sequence speech recognition,&rdquo; <em>
  arXiv e-prints</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1902.01955">pdf</a>&nbsp;]

</td>
</tr>
</table>

## Language understanding

<!-- This document was automatically generated with bibtex2html 1.98
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     /usr/bin/bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kannan2018semi">1</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Kannan, K.&nbsp;Chen, D.&nbsp;Jaunzeikare, and A.&nbsp;Rajkomar, &ldquo;Semi-Supervised
  Learning for Information Extraction from Dialogue,&rdquo; in <em>Proc.
  Interspeech</em>, 2018.
[&nbsp;<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1318.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="yavuz2018calcs">2</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Yavuz, C.&nbsp;C. Chiu, P.&nbsp;Nguyen, and Y.&nbsp;Wu, &ldquo;CaLcs: Continuously
  Approximating Longest Common Subsequence for Sequence Level Optimization,&rdquo;
  in <em>Proc. Conference on Empirical Methods in Natural Language Processing
  (EMNLP)</em>, 2018.
[&nbsp;<a href="http://aclweb.org/anthology/D18-1406">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="haghani2018s2p">3</a>]
</td>
<td class="bibtexitem">
P.&nbsp;Haghani, A.&nbsp;Narayanan, M.&nbsp;Bacchiani, G.&nbsp;Chuang, N.&nbsp;Gaur, P.&nbsp;Moreno,
  R.&nbsp;Prabhavalkar, Z.&nbsp;Qu, and A.&nbsp;Waters, &ldquo;From Audio to Semantics: Approaches
  to End-to-End Spoken Language Understanding,&rdquo; in <em>Proc. IEEE Spoken
  Language Technology Workshop (SLT)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1809.09190">pdf</a>&nbsp;]

</td>
</tr>
</table>

## Speech synthesis

<!-- This document was automatically generated with bibtex2html 1.98
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     /usr/bin/bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="shen2018natural">1</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Shen, R.&nbsp;Pang, R.&nbsp;J. Weiss, M.&nbsp;Schuster, N.&nbsp;Jaitly, Z.&nbsp;Yang, Z.&nbsp;Chen,
  Y.&nbsp;Zhang, Y.&nbsp;Wang, R.&nbsp;Skerry-Ryan, R.&nbsp;A. Saurous, Y.&nbsp;Agiomyrgiannakis, and
  Y.&nbsp;Wu, &ldquo;Natural TTS synthesis by conditioning WaveNet on mel spectrogram
  predictions,&rdquo; in <em>Proc. IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://google.github.io/tacotron/publications/tacotron2/index.html">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1703.10135">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chorowski2018styletransfer">2</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Chorowski, R.&nbsp;J. Weiss, R.&nbsp;A. Saurous, and S.&nbsp;Bengio, &ldquo;On using
  backpropagation for speech texture generation and voice conversion,&rdquo; in <em>
  Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://google.github.io/speech_style_transfer/samples.html">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1712.08363">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="jia2018multispeaker">3</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Jia, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, Q.&nbsp;Wang, J.&nbsp;Shen, F.&nbsp;Ren, Z.&nbsp;Chen, P.&nbsp;Nguyen,
  R.&nbsp;Pang, I.&nbsp;Lopez-Moreno, and Y.&nbsp;Wu, &ldquo;Transfer learning from speaker
  verification to multispeaker text-to-speech synthesis,&rdquo; in <em>Advances in
  Neural Information Processing Systems</em>, 2018.
[&nbsp;<a href="https://google.github.io/tacotron/publications/speaker_adaptation/index.html">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1806.04558">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hsu2019hierarchical">4</a>]
</td>
<td class="bibtexitem">
W.&nbsp;N. Hsu, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, H.&nbsp;Zen, Y.&nbsp;Wu, Y.&nbsp;Wang, Y.&nbsp;Cao, Y.&nbsp;Jia,
  Z.&nbsp;Chen, J.&nbsp;Shen, P.&nbsp;Nguyen, and R.&nbsp;Pang, &ldquo;Hierarchical generative modeling
  for controllable speech synthesis,&rdquo; in <em>Proc. International Conference
  on Learning Representations (ICLR)</em>, 2019.
[&nbsp;<a href="https://google.github.io/tacotron/publications/gmvae_controllable_tts/index.html">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1810.07217">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hsu2018disentangling">5</a>]
</td>
<td class="bibtexitem">
W.&nbsp;N. Hsu, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, Y.&nbsp;A. Chung, Y.&nbsp;Wang, Y.&nbsp;Wu, and J.&nbsp;Glass,
  &ldquo;Disentangling correlated speaker and noise for speech synthesis via data
  augmentation and adversarial factorization,&rdquo; in <em>NeurIPS 2018 Workshop
  on Interpretability and Robustness in Audio, Speech, and Language</em>, 2018.
[&nbsp;<a href="https://openreview.net/forum?id=Bkg9ZeBB37">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zen2019libritts">6</a>]
</td>
<td class="bibtexitem">
H.&nbsp;Zen, V.&nbsp;Dang, R.&nbsp;Clark, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, Y.&nbsp;Jia, Z.&nbsp;Chen, and Y.&nbsp;Wu,
  &ldquo;LibriTTS: A corpus derived from LibriSpeech for text-to-speech,&rdquo; in
  <em>submitted to Interspeech</em>, 2019.
[&nbsp;<a href="http://www.openslr.org/60">data</a>&nbsp;| 
<a href="https://arxiv.org/abs/1904.02882">pdf</a>&nbsp;]

</td>
</tr>
</table>

## Speech translation

<!-- This document was automatically generated with bibtex2html 1.98
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     /usr/bin/bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="weiss2017sequence">1</a>]
</td>
<td class="bibtexitem">
R.&nbsp;J. Weiss, J.&nbsp;Chorowski, N.&nbsp;Jaitly, Y.&nbsp;Wu, and Z.&nbsp;Chen,
  &ldquo;Sequence-to-sequence models can directly translate foreign speech,&rdquo; in
  <em>Proc. Interspeech</em>, 2017.
[&nbsp;<a href="https://arxiv.org/abs/1703.08581">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="jia2019leveraging">2</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Jia, M.&nbsp;Johnson, W.&nbsp;Macherey, R.&nbsp;J. Weiss, Y.&nbsp;Cao, C.&nbsp;C. Chiu, N.&nbsp;Ari,
  S.&nbsp;Laurenzo, and Y.&nbsp;Wu, &ldquo;Leveraging weakly supervised data to improve
  end-to-end speech-to-text translation,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1811.02050">pdf</a>&nbsp;]

</td>
</tr>
</table>